{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "姓名：于永夏\n",
    "学号：2022201050177\n",
    "\n",
    "利用AG_News数据库，通过输入相应文字来判别是哪个类型的文本\n",
    "类型分为四类\"World\" \"Sports\" \"Business\" \"Sci/Tec\"\n",
    "input文件夹 是存放输入数据库\n",
    "外部的test.csv和train.csv 是新生成的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. process the ag_news dataset\\n2. define the model of classification\\n3. train the model to classification the ag_news\\n4. test the model\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运算b步骤如下\n",
    "\n",
    "\"\"\"\n",
    "1. process the ag_news dataset\n",
    "2. define the model of classification\n",
    "3. train the model to classification the ag_news\n",
    "4. test the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. process the ag_news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Index\n",
    "\n",
    "\n",
    "class MyTextDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        super(MyTextDataset, self).__init__()\n",
    "        name = 'train' if train else 'test'\n",
    "        self.data_file = pd.read_csv(csv_path, encoding='utf-8', sep=',')\n",
    "        self.labels = list(self.data_file[\"label\"])\n",
    "        self.texts = list(self.data_file[\"text\"])\n",
    "        print(f\"the {name} dataset are {len(self.data_file.index)}'s numbers\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(zip(self.labels, self.texts))\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(iter(zip(self.labels, self.texts)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_file.index)\n",
    "\n",
    "\n",
    "def convert_dataset(raw_path: str, new_path: str, train: bool = True):\n",
    "    \"\"\"\n",
    "    convert the ag_news train.csv or test.csv(contain 3 columns:Class Index,Title,Description) \\\n",
    "    generate a new train.csv or test.csv(contain 2 columns:Class Index,Description)\n",
    "    notice!! the Description contains the Title\n",
    "\n",
    "    args:\n",
    "    - raw_path: input a dataset path\n",
    "    - new_path: save new train.csv or test.csv into here\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'train' if train else 'test'\n",
    "    raw_path = pathlib.Path(raw_path)\n",
    "    raw_path = list(raw_path.glob(f\"*{name}.csv\"))[0]\n",
    "    raw_path = str(raw_path)\n",
    "\n",
    "    raw_file = pd.read_csv(raw_path)\n",
    "    # print(len(raw_file.index))\n",
    "    rows = raw_file.shape[0]\n",
    "    columns = raw_file.shape[1] - 1\n",
    "\n",
    "    new_path = new_path + f\"/{name}.csv\"\n",
    "    # print(new_path)\n",
    "    new_file = pd.DataFrame(np.zeros((rows, columns)), index=np.arange(rows), columns=[\"label\", \"text\"])\n",
    "    new_file[\"label\"] = raw_file[\"Class Index\"]\n",
    "    new_file[\"text\"] = raw_file[\"Title\"]\n",
    "    new_file[\"text\"] += raw_file[\"Description\"]\n",
    "\n",
    "    # x = raw_file[\"Title\"] + raw_file[\"Description\"]\n",
    "    # print(x)\n",
    "    new_file.to_csv(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train dataset are 120000's numbers\n",
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters)Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./input\"\n",
    "destination = \"./\"\n",
    "convert_dataset(file_path, destination, train=True)\n",
    "convert_dataset(file_path, destination, train=False)\n",
    "train_dataset = MyTextDataset(\"./train.csv\")\n",
    "print(next(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. define the model of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "barch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.num_layers = 2\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "#         self.lstm = nn.LSTM(embed_dim, 64, self.num_layers, bidirectional=True)\n",
    "        self.gru = nn.GRU(embed_dim, 64, self.num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(128, num_class)\n",
    "        self.init_weights()\n",
    "#         # lstm parameters\n",
    "#         self.h0 = torch.randn(2*self.num_layers, barch_size, 128)\n",
    "#         self.c0 = torch.randn(2*self.num_layers, barch_size, 128)\n",
    "#         # gru parameters\n",
    "#         self.h1 = torch.randn(2*self.num_layers, barch_size, 256)\n",
    "#         self.c1 = torch.randn(2*self.num_layers, barch_size, 256)\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        x = self.embedding(text, offsets)\n",
    "#         x = self.lstm(x, (self.h0,self.c0))\n",
    "#         x = self.gru(x, (self.h1,self.c1))\n",
    "#         x,(_h,_c) = self.lstm(x)\n",
    "        x,_h = self.gru(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. train the model to classification the ag_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 import kit and define some environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "import time\n",
    "\n",
    "from torch.optim import optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, vocab\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "train_file_path = \"./train.csv\"\n",
    "test_file_path = \"./test.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "# Hyper parameters\n",
    "epoch = 10  # epoch\n",
    "barch_size = 64  # batch size for training\n",
    "LR = 5  # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 define some function to prepare loading dataset or training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield tokenizer(line)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "def train(dataloader, loss_fn):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = loss_fn(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = loss_fn(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train dataset are 120000's numbers\n",
      "the test dataset are 7600's numbers\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Access to the raw dataset iterators and and Generate data batch and iterator\n",
    "'''\n",
    "train_iter = MyTextDataset(train_file_path, train=True)\n",
    "test_iter = MyTextDataset(test_file_path, train=False)\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=barch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=barch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=barch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Prepare data processing pipelines  \n",
    "'''\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_file_path), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# print(vocab[\"<unk>\"])\n",
    "# print(vocab(['here', 'is', 'the', 'an', 'example']))\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Define the model\n",
    "'''\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "em_size = 64\n",
    "model = TextClassificationModel(vocab_size, em_size, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.393\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.728\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.810\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 122.84s | valid accuracy    0.838 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.859\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.870\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 131.40s | valid accuracy    0.867 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.896\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.894\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.900\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 146.22s | valid accuracy    0.886 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.909\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.910\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.912\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 143.41s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.920\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.920\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.918\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 148.76s | valid accuracy    0.884 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.936\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.939\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 167.35s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.939\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.942\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.941\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 171.64s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.943\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 164.09s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 152.97s | valid accuracy    0.898 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 149.96s | valid accuracy    0.900 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. Train the model\n",
    "'''\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, epoch + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, criterion)\n",
    "    accu_val = evaluate(valid_dataloader, criterion)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.906\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader, criterion)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. you can predict your gossip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Xiaoming like basketball!'  is a Sports news\n",
      "'Wuhan University is one of best university!' is a Sci/Tec news\n",
      "'Wuhan is located in china!' is a World news\n"
     ]
    }
   ],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text)).to(device)\n",
    "        output = model(text, torch.tensor([0]).to(device))\n",
    "        return output.argmax(1).item() + 1\n",
    "    \n",
    "ex_text_str1 = \"Xiaoming like basketball!\"\n",
    "ex_text_str2 = \"Wuhan University is one of best university!\"\n",
    "ex_text_str3 = \"Wuhan is located in china!\"\n",
    "\n",
    "print(f\"\\'{ex_text_str1}\\'  is a %s news\" % ag_news_label[predict(ex_text_str1, text_pipeline)])\n",
    "print(f\"\\'{ex_text_str2}\\' is a %s news\" % ag_news_label[predict(ex_text_str2, text_pipeline)])\n",
    "print(f\"\\'{ex_text_str3}\\' is a %s news\" % ag_news_label[predict(ex_text_str3, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6282b6e4d677aa4f061341f82854aa406aca119071d71869111bf6890f340ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
